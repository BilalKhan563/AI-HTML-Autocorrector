# BERT Model README

## Model Training:
In this section, I will explain how I trained the BERT (Bidirectional Encoder Representations from Transformers) model for correcting broken HTML code.

### Data Preparation:
To train the BERT model, I collected a dataset containing pairs of HTML code: one with errors or issues (referred to as "Bad_html"), and the other with the correct version (referred to as "Proper_html"). The dataset was cleaned to remove any unnecessary HTML tags and special characters.

### Data Split:
I split the dataset into two subsets: one for training and another for testing. Approximately 80% of the data was used for training, and the remaining 20% was reserved for testing the model's performance.

### Model Selection:
For this task, I selected the "bert-base-uncased" model, a pre-trained BERT model. This model is known for its effectiveness in understanding and generating text, making it suitable for correcting HTML code.

### Tokenization:
Before training, I tokenized the HTML code using the BERT Tokenizer, which breaks down text into smaller units called tokens. This step was essential for feeding the data into the model.

### Training Process:
The BERT model was trained using the training dataset to learn how to correct HTML code. The model underwent multiple training epochs, iteratively improving its ability to fix HTML errors.

## Model Testing:
In this section, I describe how the trained BERT model was tested on HTML code to assess its correction capabilities.

### Tokenization:
To apply the model, I tokenized the messy HTML code to convert it into a format suitable for input to the model.

### HTML Correction:
The BERT model was used to correct the messy HTML code, applying its learned knowledge to fix errors and format the HTML properly.

### Evaluation:
I compared the corrected HTML code generated by the model with the corresponding Proper_html from the testing dataset. If the model's output closely matched the expected result, it was considered successful.

## Model Parameters:
Here, I provide an overview of the BERT model's parameters and configurations used in the training process.

- Model Name: "bert-base-uncased"
- Tokenizer: BERT Tokenizer
- Maximum Sentence Length: 256 tokens
- Task: Binary classification (HTML correction)

The model training included important parameters such as learning rate, batch size, and the number of training epochs. These parameters were carefully chosen to optimize the model's learning process.

This README summarizes the training, testing, and key parameters of the BERT model used to correct HTML code. It provides insight into how the model was prepared, tested, and the specific settings used in the training process.
